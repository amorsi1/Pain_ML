{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cd06ca-17a5-48bf-9957-4bd327509562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from typing import List, Dict, Tuple\n",
    "import h5py\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb563cb-b31b-400c-91fd-031a4963bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbs_example = '/mnt/hd0/Pain_ML_data/videos/pbs/pbs-trimmed_2024-07-15_22-04-39_1-2blue'\n",
    "formalin_example = '/mnt/hd0/Pain_ML_data/videos/formalin/formalin-trimmed_2024-07-15_22-04-39_1-1blue'\n",
    "\n",
    "ex = os.path.join(pbs_example,'features.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4540bf90-9f14-4841-99f5-bd3333dedc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring file: /mnt/hd0/Pain_ML_data/videos/pbs/pbs-trimmed_2024-07-15_22-04-39_1-2blue/features.h5\n",
      "\n",
      "Group: pbs-trimmed_2024-07-15_22-04-39_1-2blue\n",
      "Number of datasets: 27\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_data\n\u001b[0;32m---> 33\u001b[0m \u001b[43mexplore_h5_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mexplore_h5_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of datasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m total_data_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal data points: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_data_points\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGroup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of datasets: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(datasets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m total_data_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal data points: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_data_points\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# def traverse_hdf5(name, node, group_data):\n",
    "#     group_name = name.split('/')[0]  # Get the top-level group name\n",
    "#     if isinstance(node, h5py.Dataset):\n",
    "#         if group_name not in group_data:\n",
    "#             group_data[group_name] = {}\n",
    "#         dataset_name = name.split('/')[-1]  # Get the dataset name\n",
    "#         group_data[group_name][dataset_name] = {\n",
    "#             'shape': node.shape,\n",
    "#             'dtype': str(node.dtype),\n",
    "#             # 'sample': node[(0,) * len(node.shape)][:5].tolist()  # Convert to list for easier printing\n",
    "#         }\n",
    "\n",
    "# def explore_h5_file(file_path):\n",
    "#     group_data = defaultdict(dict)\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         f.visititems(lambda name, node: traverse_hdf5(name, node, group_data))\n",
    "    \n",
    "#     print(f\"Exploring file: {file_path}\\n\")\n",
    "#     for group_name, datasets in group_data.items():\n",
    "#         print(f\"Group: {group_name}\")\n",
    "#         print(f\"Number of datasets: {len(datasets)}\")\n",
    "#         total_data_points = sum(info['shape'][0] for info in datasets.values())\n",
    "#         print(f\"Total data points: {total_data_points}\")\n",
    "#         print(\"Datasets:\")\n",
    "#         for dataset_name, info in datasets.items():\n",
    "#             print(f\"  - {dataset_name}\")\n",
    "#             print(f\"    Shape: {info['shape']}\")\n",
    "#             print(f\"    Type: {info['dtype']}\")\n",
    "#             # print(f\"    First few values: {info['sample']}\")\n",
    "#         print()\n",
    "#     return group_data\n",
    "\n",
    "# explore_h5_file(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb39f3f-1a0d-42db-82b6-ee854cd6513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Manager:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.file = None\n",
    "        self.group_data = defaultdict(dict)\n",
    "        self._traverse_file()\n",
    "\n",
    "    def _traverse_file(self):\n",
    "        with h5py.File(self.file_path, 'r') as f:\n",
    "            f.visititems(self._collect_datasets)\n",
    "\n",
    "    def _collect_datasets(self, name, node):\n",
    "        if isinstance(node, h5py.Dataset):\n",
    "            group_name = name.split('/')[0]\n",
    "            dataset_name = name.split('/')[-1]\n",
    "            self.group_data[group_name][dataset_name] = name  # Store full path\n",
    "\n",
    "    def _ensure_file_open(self):\n",
    "        if self.file is None or not self.file.id.valid:\n",
    "            self.file = h5py.File(self.file_path, 'r')\n",
    "\n",
    "    def close(self):\n",
    "        if self.file is not None:\n",
    "            self.file.close()\n",
    "            self.file = None\n",
    "\n",
    "    def get_dataset(self, group_name, dataset_name) -> np.array:\n",
    "        # Extracts given dataset from a specific group\n",
    "        self._ensure_file_open()\n",
    "        full_path = self.group_data[group_name][dataset_name]\n",
    "        return self.file[full_path]\n",
    "\n",
    "    def get_dataset_for_all_groups(self, dataset_name) -> Dict[str,np.array]:\n",
    "        dataset_dict = {}\n",
    "        for group_name in self.group_data.keys():\n",
    "            dataset = self.get_data(group_name, dataset_name)\n",
    "            dataset_dict[group_name] = dataset\n",
    "        return dataset_dict\n",
    "\n",
    "    def get_data(self, group_name, dataset_name, slice=None):\n",
    "        dataset = self.get_dataset(group_name, dataset_name)\n",
    "        if slice is None:\n",
    "            return dataset[:]\n",
    "        return dataset[slice]\n",
    "\n",
    "    def get_metadata(self, group_name, dataset_name):\n",
    "        dataset = self.get_dataset(group_name, dataset_name)\n",
    "        return {\n",
    "            'shape': dataset.shape,\n",
    "            'dtype': str(dataset.dtype),\n",
    "            'chunks': dataset.chunks,\n",
    "            'compression': dataset.compression\n",
    "        }\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cd409f2-bd3c-4914-9263-33b4335dcdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups: ['pbs-trimmed_2024-07-15_22-04-39_1-2blue']\n",
      "\n",
      "Datasets in pbs-trimmed_2024-07-15_22-04-39_1-2blue: ['ankle_distance', 'background_luminance', 'both_front_paws_lifted', 'cheek_distance', 'chest_head_angle', 'distance_delta', 'fps', 'frame_count', 'front_left_luminance', 'front_paws_distance', 'front_right_luminance', 'hind_left_luminance', 'hind_paws_distance', 'hind_right_luminance', 'hip_chest_angle', 'hip_sternumtail_distance', 'hip_tailbase_distance', 'hip_tailbase_hlpaw_angle', 'hip_tailbase_hrpaw_angle', 'hip_width', 'neck_snout_distance', 'paw_guarding', 'shoulder_width', 'sternumhead_neck_distance', 'sternumtail_sternumhead_distance', 'tail_hip_angle', 'tailbase_tailtip_distance']\n"
     ]
    }
   ],
   "source": [
    "file_path = ex\n",
    "with HDF5Manager(file_path) as hdf:\n",
    "    # List all groups\n",
    "    print(\"Groups:\", list(hdf.group_data.keys()))\n",
    "    # Choose a group\n",
    "    group_name = list(hdf.group_data.keys())[0]\n",
    "    print(f\"\\nDatasets in {group_name}:\", list(hdf.group_data[group_name].keys()))\n",
    "    # Get data\n",
    "    data = hdf.get_data(group_name, 'both_front_paws_lifted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f4e92cc-4879-46a0-84ab-eccc84acde9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18da0f50-8b29-452b-a807-cfda5b4a74f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([False,  True]), array([13206,   117]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7905a629-7d34-4324-b766-5e3a49fb0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pain_datasets = ['paw_guarding','both_front_paw_lifted'] #potentially add ankle distance \n",
    "# add luminance_logratio after rerunning analysis-public on all recordings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336dc76c-351d-46a6-a055-0217cbc58159",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
